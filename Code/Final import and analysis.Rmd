---
title: "Final data ingest and analysis"
author: "Abby Lewis"
date: "10/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(googlesheets4)
library(googledrive)
library(tidyverse)
library(lubridate)
library(colorspace)
```



7 Mar 2021: This is an old file- do not use




```{r}
files = drive_ls(path = "My projects/Forecasting Analysis/Abstract screening and matrix analysis")
files = files[files$name %in% c("for_dup_compare","for_dup_compare2"),]

for(i in 1:nrow(files)){
  matrix <- data.frame(t(read_sheet(files$id[i], col_types = "c")))
  colnames(matrix)<-matrix[1,]
  matrix <- matrix[2:nrow(matrix),]
  write.csv(matrix, paste0("../Data/Finished_dups/",files$name[i], ".csv"))
}

new_names <- c("PaperID", "Reviewer","Title","Authors","DOI","Journal",
                    "Year","FirstComment","SecondComment","ThirdComment","Location","Include_yn",
                    "Header1","Coauthors_nonAc","Coauthors_gov","Spat_scale","Coords",
                    "Ecosystem","Class","Vars_n","Vars_ident",
                    "Header2","Model_dim","Model_type","Model_desc","Covariate_classes",
                    "Random_effects_yn","Latent_vars_yn","Ens_within_model_yn",
                    "Ens_members_n","Ens_of_models_yn","Models_n","Multiple_approaches_yn",
                    "Approach_n","Null_yn","Null_n","Null_type","Horiz_days",
                    "Time_step_days","How_often_days","Spat_step","Training_yn",
                    "Header3","Iterative_yn","DA_type","DA_latency_days","Uncert_category",
                    "Uncert_technique","Uncert_source","Uncert_obs_yn","Uncert_partition_yn",
                    "Uncert_ic_yn","Uncert_driver_yn","Uncert_param_yn","Uncert_process_yn",
                    "Uncert_other_yn","Uncert_dom","Uncert_describe","Scenarios_yn",
                    "Scenarios_n","Forecast_eval","Forecast_eval_shown","Eval_metrics","R2_possible",
                    "Eval_mult_horiz_yn","Cycles_eval_n","Cycles_total_n","Forecast_horiz_null_days",
                    "Header4","Driver_lat_max_days","Driver_lat_min_days","Data_coverage",
                    "Automated_yn","Archiving_yn","Repository","Drivers_published","Header5", "End_user_yn",
                    "End_user_partnership","Used_by_stakeholder_yn",
                    "Delivery_method_yn","Delivery_method","Ethical_considerations")

files2 <- list.files("../Data/Finished_dups")
comb <- read_csv(paste0("../Data/Finished_dups/",files2[1]), col_types = paste0(rep("c",83),collapse = ""), col_names = new_names, skip = 1)

for (i in 2:length(files2)){
  new <- read_csv(paste0("../Data/Finished_dups/",files2[i]), col_types = paste0(rep("c",83),collapse = ""), col_names = new_names, skip = 1)
  comb = comb%>%
    full_join(new)
}

comb = comb%>%
  select(-Header1,-Header2,-Header3,-Header4,-Header5)

comb$Author_n <- str_count(comb$Authors, pattern = ";")+1
comb$Met_covar_yn <- grepl("meteorological", comb$Covariate_classes, ignore.case = T)*1
comb$Phys_covar_yn <- grepl("physical", comb$Covariate_classes, ignore.case = T)*1
comb$Bio_covar_yn <- grepl("biological", comb$Covariate_classes, ignore.case = T)*1
comb$Chem_covar_yn <- grepl("chemical", comb$Covariate_classes, ignore.case = T)*1

comb2 <- comb[!duplicated(comb$Title),]#Filtering out the second copy of each paper (should be identical)

complete_forecasts <- comb2[is.na(comb2$Include_yn)|tolower(comb2$Include_yn)=="yes"|comb2$Include_yn=="1",]
exclude <- comb2[!(is.na(comb2$Include_yn)|tolower(comb2$Include_yn)=="yes"|comb2$Include_yn=="1"),]
complete_forecasts$Year <- as.numeric(complete_forecasts$Year)
complete_forecasts[complete_forecasts=="n"] <- "0"
complete_forecasts[complete_forecasts=="y"] <- "1"

write.csv(complete_forecasts, "../Data/complete_dataset.csv", row.names = F)

complete_forecasts$Null_type[complete_forecasts$Null_yn==1]
10/21
```



Papers per year
```{r}
complete_forecasts <- read.csv("../Data/complete_dataset.csv")
eco_total <- read.csv("../Data/ecology_results.txt", sep = "\t")

years <- as.data.frame(seq(min(complete_forecasts$Year, na.rm = T)-1,max(complete_forecasts$Year, na.rm = T)))
colnames(years)<- "Year"
for_hist1 <- complete_forecasts%>%
  group_by(Year)%>%
  summarize(total = n())
for_hist <- years%>%
  left_join(for_hist1)%>%
  filter(Year !=2020)
for_hist$total[is.na(for_hist$total)] <- 0

text = data.frame(text = c("First\nhalf\n(n = 82)","Second\nhalf\n(n = 96)"),x = c(2012,2015),y = c(23.5,23.5))

jpeg("../Figures/Pubs_year_noEco.jpeg", width = 7, height = 6, units = "in",res = 300)
complete_forecasts%>%
  ggplot()+
  geom_histogram(aes(x = Year),binwidth = 1)+
  xlim(1930,2020)+
  theme_bw()+
  #geom_vline(aes(xintercept=2013.5))+
  xlab("Year")+
  ylab("Number of near-term ecological forecasts")+
  #geom_text(aes(x = x, y=y,label=text),text, hjust = c("right","left"),lineheight = .9)+
  ylim(0,25)
dev.off()

eco_total%>%
  ggplot(aes(x=as.numeric(Publication.Years), y = records))+
  geom_line()+
  xlim(1930,2019)
  

for_2000 <- for_hist$total[for_hist$Year == 2010]
for_max <- max(for_hist$total)
eco_2000 <- eco_total$records[eco_total$Publication.Years == 2010]
eco_max = eco_2000*for_max/for_2000

options(scipen=5)
jpeg("../Figures/Pubs_year.jpeg", width = 7, height = 6, units = "in",res = 300)
par(mar = c(5, 5, 3, 5))
#hist(complete_forecasts$PY, breaks = 50, main = "",xlab = "Year", ylab = "ecological forecasts")
plot(eco_total$Publication.Years[-1],eco_total$records[-1], type = "l",xaxt = "n", yaxt = "n",ylab = "", xlab = "", col = "red", ylim = c(0, eco_max), xlim = range(for_hist$Year))
axis(2, col.ticks = "red",col.axis = "red")
par(new = T)
plot(for_hist$Year, for_hist$total, main = "",yaxt = "n",ylab = "",xlab = "Year", type = "l", ylim = c(0,for_max))
axis(4)
legend("topleft", c("Near-term ecological forecasts", "Publications in ecology journals"),
       col = c("black", "red"), lty = 1)
mtext("Publications in ecology journals", side = 2, line = 3, col = "red")
mtext("Near-term ecological forecasts", side = 4, line = 3)
dev.off()

for_hist%>%
  left_join(eco_total[-1,]%>%mutate(Publication.Years = as.numeric(Publication.Years)), by = c("Year"="Publication.Years"))%>%
  select(-X..of.573171)%>%
  filter(Year>1950)%>%
  mutate(Percent = total/records*100)%>%
  ggplot(aes(x = Year, y = Percent))+
  ylab("NEF papers as a percentage of ecological papers")+
  geom_line()

complete_forecasts[complete_forecasts$Forecast_eval== "1",]
complete_forecasts$Uncert_source[complete_forecasts$Uncert_category == "no"] <- NA
complete_forecasts$Driver <- grepl("driver|meteo", complete_forecasts$Uncert_source, ignore.case = T)*1
complete_forecasts$Process <- grepl("process", complete_forecasts$Uncert_source, ignore.case = T)*1
complete_forecasts$Param <- grepl("param", complete_forecasts$Uncert_source, ignore.case = T)*1
complete_forecasts$IC <- grepl("init|ic", complete_forecasts$Uncert_source, ignore.case = T)*1
complete_forecasts$Obs <- grepl("obs", complete_forecasts$Uncert_source, ignore.case = T)*1

complete_forecasts%>%
  filter(!Uncert_category=="no")%>%
  summarize(driv = sum(Driver)/n(),
            proc = sum(Process)/n(),
            param = sum(Param)/n(),
            ic = sum(IC)/n(),
            obs = sum(Obs)/n())

complete_forecasts$Vars_ident[grepl("ET |evap",complete_forecasts$Vars_ident,ignore.case = T)]
complete_forecasts$Vars_ident[grepl("pollen",complete_forecasts$Vars_ident,ignore.case = T)]
complete_forecasts$Vars_ident[grepl("diatom|cyano|oscillatoria|asterionella|desmus|anab|aphaniz|synecho|cyclo|stephanodiscus|raciborskii|circinale",complete_forecasts$Vars_ident, ignore.case = T)]
complete_forecasts$Vars_ident[grepl("chl",complete_forecasts$Vars_ident, ignore.case = T)]
complete_forecasts$Vars_ident[grepl("alg|hab |phyto",complete_forecasts$Vars_ident, ignore.case = T)]
complete_forecasts$Vars_ident[!grepl("yield",complete_forecasts$Vars_ident, ignore.case = T)]
complete_forecasts$Vars_ident[grepl("catch|fish|pollock|salmon|anchovy|walleye|herring|trout|carp",complete_forecasts$Vars_ident, ignore.case = T)] #except not lobster catch, shellfish harvesting, rusty crayfish distribution

(14+14+10)/178
```


```{r}
complete_forecasts%>%
  ggplot(aes(x = Coauthors_nonAc!=0, fill =  End_user_yn))+
  geom_bar()+
  theme_bw()

complete_forecasts%>%
  ggplot(aes(x = Coauthors_gov!=0, fill =  End_user_yn))+
  geom_bar()+
  theme_bw()
```

Journals
```{r}
data1 <- read_csv("../Data/savedrecs_001_500.csv")
data2 <- read_csv("../Data/savedrecs_501_1000.csv")
data3 <- read_csv("../Data/savedrecs_1001_1500.csv")
data4 <- read_csv("../Data/savedrecs_1501_2000.csv")
data5 <- read_csv("../Data/savedrecs_2001_2500.csv")
data6 <- read_csv("../Data/savedrecs_2501_2956.csv")
data7 <- read_csv("../Data/papers_citing_forecasts_full.csv")
data8 <- read_csv("../Data/referenced_papers_forecasts.csv")
data9 <- read_csv("../Data/citing_ed_foundLate.csv")%>%
  rename(TI="Article Title",
         SO = "Source Title",
         LA = "Language")
data_full <- data1%>% select(TI, SO, LA)%>%
  full_join(data2%>%  select(TI, SO, LA))%>%
  full_join(data3%>%  select(TI, SO, LA))%>%
  full_join(data4%>%  select(TI, SO, LA))%>%
  full_join(data5%>%  select(TI, SO, LA))%>%
  full_join(data6%>%  select(TI, SO, LA))%>%
  full_join(data7%>%  select(TI, SO, LA))%>%
  full_join(data8%>%  select(TI, SO, LA))%>%
  full_join(data9%>%  select(TI, SO, LA))

summary(as.factor(data_full$LA))/nrow(data_full)

journals = complete_forecasts%>%
  left_join(data_full, by = c("Title"="TI"))

jpeg("../Figures/EF_journals.jpg",res=300,width = 20, height = 12, units = "in")
journals%>%
  ggplot(aes(y = SO))+
  geom_bar()+
  scale_y_discrete(limits = c(rev(levels(as.factor(journals$SO))),NA))
dev.off()

length(unique(journals$SO)) #114 different journals
nrow(complete_forecasts)
```

Map
```{r}
complete_forecasts <- read.csv("../Data/complete_dataset.csv")
WorldData <- map_data('world') %>% 
  fortify()
safe_colorblind_palette <- c("#661100", "#AA4499", "#DDCC77", "#117733","#44AA99","#999933","#332288", "#88CCEE", "#CC6677","#888888")


for_map <- complete_forecasts
coords <- for_map$Coords
for(i in 1:(max(str_count(for_map$Coords, ";"), na.rm = T)+1)){
  stops <- regexpr(pattern = ";",coords)
  stops[stops == -1] <- 10000L
  coords_i <- substr(coords, start = 1, stop = stops-1)
  for_map[nchar(coords_i)>0&!is.na(coords_i), paste0("Long",i)] <- as.numeric(sub("-*[0-9]+.[0-9]+,","",coords_i))[nchar(coords_i)>0&!is.na(coords_i)]
  for_map[nchar(coords_i)>0&!is.na(coords_i), paste0("Lat",i)] <- as.numeric(sub(", -*[0-9]+.[0-9]+","",coords_i))[nchar(coords_i)>0&!is.na(coords_i)]
  inds <- regexpr(pattern = ";",coords)
  inds[inds == -1]<-NA
  coords <- substr(coords, start = inds + 2, stop = 10000L)
}

for_map2 = for_map%>%
  pivot_longer(cols = num_range("Lat", 1:max(str_count(for_map$Coords, ";")+1, na.rm = T)), names_to = "Lat_num", values_to= "Lat")%>%
  pivot_longer(cols = num_range("Long", 1:max(str_count(for_map$Coords, ";")+1, na.rm = T)), names_to = "Long_num", values_to = "Long")%>%
  filter(sub("[a-z]+","",Lat_num)==sub("[a-z]+","",Long_num, ""))%>%
  filter(!Spat_scale == "global")

north_analysis <- for_map2$Lat[!is.na(for_map2$Lat)]
sum(north_analysis>0)/length(north_analysis)
sum(complete_forecasts$Spat_scale=="regional")/nrow(complete_forecasts)
sum(complete_forecasts$Class=="organismal"|complete_forecasts$Class=="both")/nrow(complete_forecasts)
sum(complete_forecasts$Ecosystem=="agricultural")/nrow(complete_forecasts)

#library(scales)
#colors <- hue_pal()(10)
#colors = c(colors, "#7f7f7f")
colors = lighten(safe_colorblind_palette, amount = .2)

for_map2$Region <- NA
for_map2$Region[for_map2$Spat_scale %in% c("regional","national")] <- 1
for_map2$Region[for_map2$Spat_scale %in% c("point","multipoint")]<-0

map <- ggplot() +
  geom_map(data = WorldData, map = WorldData,
                  aes(map_id=region),
                  fill = "white", colour = "#7f7f7f", size=0.5)+
  coord_map("rectangular", lat0=0, xlim=c(-180,180))+
  geom_point(data = for_map2%>%filter(Region == 1), aes(Long, Lat, fill = Ecosystem), shape = 21, color = "white", show.legend = F, size = 8, alpha  =.3, stroke = .5)+
  geom_point(data = for_map2%>%filter(Region == 0), aes(Long, Lat, fill = Ecosystem), shape = 21, color = "white", show.legend = F, size = 4, alpha  =1, stroke = .5)+
  theme_bw() + xlab(NULL) + ylab(NULL) +
  scale_fill_manual(breaks = c("agricultural", "atmosphere", "desert", "forest", "freshwater", "grassland", "marine", "tundra", "urban", "other", "NA"), values = colors, na.translate = T, na.value = "grey50", name = "Ecosystem")+
  #scale_shape_manual(values = c(21,23), breaks = c("point","multipoint"), name = "Spatial extent")+
  theme(
    plot.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    axis.ticks = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    legend.position = "bottom",
    legend.direction = "horizontal")+
  guides(fill = F)
  #guides(fill = guide_legend(override.aes=list(shape=21)))


complete_forecasts$Ecosystem[is.na(complete_forecasts$Ecosystem)]<-"NA"
complete_forecasts$Ecosystem <- factor(complete_forecasts$Ecosystem, levels = c("agricultural","desert","grassland","forest","atmosphere","freshwater","marine","tundra","urban","other","NA"))
complete_forecasts$Spat_scale<- factor(complete_forecasts$Spat_scale, levels = c("point","multipoint","regional","national","global"))
extent <- complete_forecasts%>%
  ggplot(aes(x = Spat_scale, fill = Ecosystem))+
  scale_fill_manual(breaks = c("agricultural", "atmosphere", "desert", "forest", "freshwater", "grassland", "marine", "tundra", "urban", "other", "NA"), values = colors, na.translate = T, na.value = "grey50", name = "Ecosystem")+
  xlab("Spatial extent")+
  geom_bar(show.legend = F)+
  ylab("Number of papers")+
  theme_light()

class <- complete_forecasts%>%
  ggplot(aes(x = Class, fill = Ecosystem))+
  geom_bar()+
  scale_fill_manual(breaks = c("agricultural", "atmosphere", "desert", "forest", "freshwater", "grassland", "marine", "tundra", "urban", "other", "NA"), values = colors, na.translate = T, na.value = "grey50", name = "Ecosystem",guide = guide_legend(ncol = 2))

g_legend<-function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}

legend_fill <- g_legend(class)

class <- complete_forecasts%>%
  ggplot(aes(x = Class, fill = Ecosystem))+
  geom_bar(show.legend = F)+
  scale_fill_manual(breaks = c("agricultural", "atmosphere", "desert", "forest", "freshwater", "grassland", "marine", "tundra", "urban", "other", "NA"), values = colors, na.translate = T, na.value = "grey50", name = "Ecosystem")+
  ylab("Number of papers")+
  theme_light()

library(ggpubr)
jpeg("../Figures/Map_composite.jpeg",width = 9, height = 6, units = "in", res = 300)
ggarrange(map, ggarrange(extent, class, legend_fill, labels = c("B","C"), label.y = c(1.2,1.2), nrow = 1, widths = c(1,1,.7)),nrow = 2, heights = c(6,3), labels = "A")
dev.off()

for_map2%>%
  filter(!is.na(Lat))

map
light
none
```

Group by year
```{r}
#Tried to get more specific dates but one of the papers is published in a book that literally doesn't have a month so...
#dates = read.csv("savedrecs_1001_1500.csv")%>%
#  select(TI,PD,PY)
#dates$PD2 = substr(gsub("[0-9]+","",gsub("-","",dates$PD)),1,3)
#dates$PD2[nchar(dates$PD2)==0]<-NA
#dates$Date <- as.Date(NA)
#dates$Date[!is.na(dates$PD2)] = as.Date(tolower(paste("01",dates$PD2[!is.na(dates$PD2)],dates$PY[!is.na(dates$PD2)])), format = "%d %B %Y")
#
#complete_forecasts%>%
#  left_join(dates,by = c("Title"="TI"))%>%
#  filter(Year==2014)

n_breaks = 2
breaks <- quantile(as.numeric(complete_forecasts$Year), probs = seq(0,1,length.out = n_breaks+1)) 
breaks <- floor(breaks)
#breaks = c(1932,2016,2021)
breaks[length(breaks)] <- breaks[length(breaks)]+1
for(i in n_breaks:1){
  complete_forecasts$Quantile[complete_forecasts$Year <= breaks[i+1]-1] <- paste0(breaks[i], " to ", breaks[i+1]-1)
}

complete_forecasts$Automated_bin <- 0
complete_forecasts$Automated_bin[complete_forecasts$Automated_yn %in% c("At least one data stream","Yes all data streams")]<-1

time_graph = complete_forecasts%>%
  group_by(Quantile)%>%
  summarise(Uncert_yn = sum(Uncert_category %in% c("data_driven", "propagates", "assimilates")), #If you get rid of contains here there is a more obvious pattern
            Null_yn = sum(as.numeric(Null_yn), na.rm = T),
            Eval_yn = sum(as.numeric(Forecast_eval_shown), na.rm = T),
            Iterative_yn = sum(as.numeric(Iterative_yn), na.rm = T),
            Archiving_yn = sum(as.numeric(Archiving_yn), na.rm = T),
            End_user_yn = sum(as.numeric(End_user_yn), na.rm = T),
            #Proc = sum(as.numeric(Model_type=="process-based"), na.rm = T),
            Auto_yn = sum(Automated_bin, na.rm = T),
            #Non_ac = sum(Coauthors_nonAc>0),
            #Multiple_approaches_yn = sum(as.numeric(Multiple_approaches_yn), na.rm = T),
            Partition_yn = sum(as.numeric(Uncert_partition_yn), na.rm = T),
            Data_yn = sum(as.numeric(Drivers_published), na.rm  =T),
            Count = n())

#time_graph%>%
#  pivot_longer(seq(2,ncol(time_graph)-1))%>%
#  mutate(Percent = value/Count*100)%>%
#  ggplot(aes(x = Quantile, y = Percent, fill = name))+
#  geom_bar(stat = "identity", position = "dodge", color = "black")+
#  theme_bw()+
#  geom_text(aes(y = 90, label = paste0("n = ",Count)))

time_graph2 = time_graph%>%
  pivot_longer(seq(2,ncol(time_graph)-1))%>%
  mutate(Percent = value/Count*100)

# Baseline
# Assess forecast skill
# Data-driven uncertainty
# Management Tier
# Identify an end user
# Iterative forecasts
# Automated forecasting workflow
# Best Scientific Practices
# FAIR data and code used to create the forecast
# Archive forecasts
# Null model comparison
# Uncertainty partitioning


time_graph2$name = factor(time_graph2$name, levels = c("Uncert_yn", "Eval_yn", "End_user_yn", "Iterative_yn", "Auto_yn", "Data_yn", "Archiving_yn","Null_yn", "Partition_yn"))

time_graph2$Tier <- "Baseline"
time_graph2$Tier[time_graph2$name %in% c("End_user_yn", "Iterative_yn", "Auto_yn")] <- "Decision Support"
time_graph2$Tier[time_graph2$name %in% c("Data_yn", "Archiving_yn","Null_yn", "Partition_yn")] <- "Science"


jpeg("../Figures/Forecasts_over_time.jpeg",width = 10, height = 4, units = "in", res = 300)
time_graph2%>%
  ggplot(aes(x = name, y = Percent, fill = Quantile))+
  geom_bar(stat = "identity", position = "dodge", color = "black")+
  xlab("")+
  ylab("Percent of papers")+
  scale_x_discrete(breaks = c("Uncert_yn", "Eval_yn", "End_user_yn", "Iterative_yn", "Auto_yn", "Data_yn", "Archiving_yn","Null_yn", "Partition_yn"), labels = c("Data-driven \nuncertainty","Forecast \nevaluation \nin paper", "End user \nspecified","Iterative \nforecasts","Workflow \nAutomation","Data availability \nspecified in paper","Forecast \narchiving", "Null model \ncomparison", "Uncertainty \npartitioned"))+
  theme_bw()+
  scale_fill_discrete(breaks = unique(time_graph2$Quantile), labels = paste0(unique(time_graph2$Quantile)," (n = ",unique(time_graph2$Count),")"), name = "Years")+
  theme(#legend.key.height=unit(2,"line"),
        legend.position = "bottom",
        legend.title = element_blank())+
  facet_grid(~Tier, scales = "free_x", space = "free_x")+
  ylim(0,100)
dev.off()

time_graph2$name = factor(time_graph2$name, levels = rev(c("Uncert_yn", "Eval_yn", "End_user_yn", "Iterative_yn", "Auto_yn", "Data_yn", "Archiving_yn","Null_yn", "Partition_yn")))
time_graph2$Quantile = factor(time_graph2$Quantile, levels = rev(unique(time_graph2$Quantile)))

jpeg("../Figures/Forecasts_over_time_vertical.jpeg",width = 5, height = 7, units = "in", res = 300)
time_graph2%>%
  ggplot(aes(y = name, x = Percent, fill = Quantile))+
  geom_bar(stat = "identity", position = "dodge", color = "black")+
  ylab("")+
  xlab("Percent of papers")+
  scale_y_discrete(breaks = c("Uncert_yn", "Eval_yn", "End_user_yn", "Iterative_yn", "Auto_yn", "Data_yn", "Archiving_yn","Null_yn", "Partition_yn"), labels = c("Include data-driven \nuncertainty", "Assess and report \nforecast skill","Identify an end user","Make iterative \nforecasts","Develop automated \nforecasting workflows","Make data available","Archive forecasts", "Use null model \ncomparisons", "Parition uncertainty"))+
  theme_bw()+
  scale_fill_discrete(breaks = unique(time_graph2$Quantile), labels = paste0(unique(time_graph2$Quantile)," (n = ",unique(time_graph2$Count),")"), name = "Years")+
  theme(legend.title = element_blank(),
        legend.position = "bottom")+
  facet_grid(rows = vars(Tier), scales = "free_y", space = "free_y")+
  xlim(0,100)
dev.off()

#
#time_graph2$name = factor(time_graph2$name, levels = rev(c("Uncert_yn", "Iterative_yn", #"End_user_yn", "Null_yn", "Archiving_yn", "Eval_yn", "Auto_yn", #"Non_ac","Multiple_approaches_yn","Proc")))
#
#jpeg("Forecasts_over_time_vertical.jpeg",width = 7, height = 5, units = "in", res = 300)
#time_graph2%>%
#  ggplot(aes(y = name, x = Percent, fill = Quantile))+
#  geom_bar(stat = "identity", position = "dodge", color = "black")+
#  xlab("Percent of papers")+
#  ylab("")+
#  scale_y_discrete(breaks = #c("Uncert_yn","Iterative_yn","End_user_yn","Null_yn","Archiving_yn","Eval_yn", #"Auto_yn","Non_ac","Multiple_approaches_yn","Proc"), labels = c("Data-driven \nuncertainty", #"Iterative \nforecasts","End user \nspecified","Null model \ncomparison","Forecast #\narchiving","Forecast \nevaluation \nin paper", "Workflow \nAutomation","Coauthor \nfrom outside #\nacademia","Multiple \napproaches \ncompared","Process-based \nmodel"))+
#  theme_light()+
#  scale_fill_discrete(breaks = unique(time_graph2$Quantile), labels = #paste0(unique(time_graph2$Quantile),"; n = ",unique(time_graph2$Count)), name = "Years")
#dev.off()
#Add automation?
#Data archiving?

time_graph2%>%
  group_by(name)%>%
  summarize(pct_1 = min(Percent),
            pct_2 = max(Percent),
            dif = pct_2/(pct_1+.0000000000001))%>%
  arrange(-dif)
```

Moving average graph
```{r}
start = min(complete_forecasts$Year[complete_forecasts$Year != 1932])
moving_average <- complete_forecasts%>%
    filter(Year < start+2)%>%
    summarise(Uncert_yn = sum(Uncert_category %in% c("data_driven", "propagates", "assimilates")), #If you get rid of contains here there is a more obvious pattern
            Null_yn = sum(as.numeric(Null_yn), na.rm = T),
            Eval_yn = sum(as.numeric(Forecast_eval_shown), na.rm = T),
            Iterative_yn = sum(as.numeric(Iterative_yn), na.rm = T),
            Archiving_yn = sum(as.numeric(Archiving_yn), na.rm = T),
            End_user_yn = sum(as.numeric(End_user_yn), na.rm = T),
            #Proc = sum(as.numeric(Model_type=="process-based"), na.rm = T),
            Auto_yn = sum(Automated_bin, na.rm = T),
            #Non_ac = sum(Coauthors_nonAc>0),
            #Multiple_approaches_yn = sum(as.numeric(Multiple_approaches_yn), na.rm = T),
            Partition_yn = sum(as.numeric(Uncert_partition_yn), na.rm = T),
            Data_yn = sum(as.numeric(Drivers_published), na.rm  =T),
            Count = n())%>%
  mutate(Year = start)
for(i in (start+1):max(complete_forecasts$Year)){
  window <- complete_forecasts%>%
    filter(Year >= i-2,
           Year <= i+2)%>%
    summarise(Uncert_yn = sum(Uncert_category %in% c("data_driven", "propagates", "assimilates")), #If you get rid of contains here there is a more obvious pattern
            Null_yn = sum(as.numeric(Null_yn), na.rm = T),
            Eval_yn = sum(as.numeric(Forecast_eval_shown), na.rm = T),
            Iterative_yn = sum(as.numeric(Iterative_yn), na.rm = T),
            Archiving_yn = sum(as.numeric(Archiving_yn), na.rm = T),
            End_user_yn = sum(as.numeric(End_user_yn), na.rm = T),
            #Proc = sum(as.numeric(Model_type=="process-based"), na.rm = T),
            Auto_yn = sum(Automated_bin, na.rm = T),
            #Non_ac = sum(Coauthors_nonAc>0),
            #Multiple_approaches_yn = sum(as.numeric(Multiple_approaches_yn), na.rm = T),
            Partition_yn = sum(as.numeric(Uncert_partition_yn), na.rm = T),
            Data_yn = sum(as.numeric(Drivers_published), na.rm  =T),
            Count = n())%>%
    mutate(Year = i)
  moving_average = moving_average%>%
    full_join(window)
}


moving_average_graph = moving_average%>%
  pivot_longer(seq(1,ncol(time_graph)-2))%>%
  mutate(Percent = value/Count*100)

moving_average_graph%>%
  filter(Count>10)%>%
  ggplot(aes(x = Year, y = Percent, color = name))+
  geom_line()+
  theme_bw()
```
Logistic regression
```{r}
complete_forecasts <- read.csv("../Data/complete_dataset.csv")
complete_forecasts$it_no_update_ic <- complete_forecasts$Iterative_yn==1&!complete_forecasts$DA_type%in%c("update IC","autoregressive, so new observed data become predictors for the next timestep","UNK","NA","autoregressive, uses previous observations to predict value at next timestep","previous ten days of driver data used as predictor in model, so it is updating iteratively")
#complete_forecasts$Iterative_yn<-complete_forecasts$it_no_update_ic
complete_forecasts$DA_type[complete_forecasts$it_no_update_ic]
complete_forecasts$Automated_bin <- 0
complete_forecasts$Automated_bin[complete_forecasts$Automated_yn %in% c("At least one data stream","Yes all data streams")]<-1
complete_forecasts$Uncert_yn <- 0
complete_forecasts$Uncert_yn <- as.numeric(complete_forecasts$Uncert_category %in% c("data_driven", "propagates", "assimilates"))
sum(complete_forecasts$Uncert_category == "contains")/nrow(complete_forecasts)
sum(complete_forecasts$Uncert_category == "contains")/sum(!complete_forecasts$Uncert_category %in% c("data_driven", "propagates", "assimilates"))

complete_forecasts$Forecast_eval_shown[complete_forecasts$Forecast_eval == 0] <- 0
complete_forecasts%>%
  summarize(aut = sum(Automated_bin)/n(),
            eva = sum(Forecast_eval_shown)/n(),
            unc = sum(Uncert_yn)/n(),
            ite = sum(Iterative_yn)/n(),
            arc = sum(Archiving_yn)/n(),
            use = sum(End_user_yn)/n(),
            dat = sum(Drivers_published)/n(),
            nul = sum(Null_yn)/n(),
            prt = sum(Uncert_partition_yn)/n())

total = complete_forecasts%>%
  group_by(Title)%>%
  summarize(aut = sum(Automated_bin)/n(),
            eva = sum(Forecast_eval_shown)/n(),
            unc = sum(Uncert_yn)/n(),
            ite = sum(Iterative_yn)/n(),
            arc = sum(Archiving_yn)/n(),
            use = sum(End_user_yn)/n(),
            dat = sum(Drivers_published)/n(),
            nul = sum(Null_yn)/n(),
            prt = sum(Uncert_partition_yn)/n(),
            Total = sum(aut,eva,unc,ite,arc,use,dat,nul,prt))
hist(total$Total, breaks = 8)
summary(as.factor(total$Total))
mean(total$Total)
median(total$Total)

complete_forecasts = complete_forecasts%>%
  filter(Year>1932)

aut = glm(Automated_bin~Year, data = complete_forecasts, family = "binomial")
eva = glm(Forecast_eval_shown~Year, data = complete_forecasts, family = "binomial")
unc = glm(Uncert_yn~Year, data = complete_forecasts, family = "binomial")
ite = glm(Iterative_yn~Year, data = complete_forecasts, family = "binomial")
arc = glm(Archiving_yn~Year, data = complete_forecasts, family = "binomial") #p = 0.025
use = glm(End_user_yn~Year, data = complete_forecasts, family = "binomial")
dat = glm(Drivers_published~Year, data = complete_forecasts, family = "binomial") #p < 0.001
nul = glm(Null_yn~Year, data = complete_forecasts, family = "binomial")
prt = glm(Uncert_partition_yn~Year, data = complete_forecasts, family = "binomial")
results = data.frame(var2 = "Include data driven uncertainty")%>%
  full_join(data.frame(summary(unc)$coefficients, var2 = NA)%>% mutate(var = "Include data driven uncertainty"))%>%
  full_join(data.frame(var2 = "Assess and report forecast skill"))%>%
  full_join(data.frame(summary(eva)$coefficients) %>% mutate(var = "Assess and report forecast skill"))%>%
  full_join(data.frame(var2 = "Identify an end user"))%>%
  full_join(data.frame(summary(use)$coefficients) %>% mutate(var = "Identify an end user"))%>%
  full_join(data.frame(var2 = "Make iterative forecasts"))%>%
  full_join(data.frame(summary(ite)$coefficients) %>% mutate(var = "Make iterative forecasts"))%>%
  full_join(data.frame(var2 = "Develop automated forecasting workflows"))%>%
  full_join(data.frame(summary(aut)$coefficients) %>% mutate(var = "Develop automated forecasting workflows"))%>%
  full_join(data.frame(var2 = "Make data available"))%>%
  full_join(data.frame(summary(dat)$coefficients) %>% mutate(var = "Make data available"))%>%
  full_join(data.frame(var2 = "Archive forecasts"))%>%
  full_join(data.frame(summary(arc)$coefficients) %>% mutate(var = "Archive forecasts"))%>%
  full_join(data.frame(var2 = "Use null model comparisons"))%>%
  full_join(data.frame(summary(nul)$coefficients) %>% mutate(var = "Use null model comparisons"))%>%
  full_join(data.frame(var2 = "Partition uncertainty"))%>%
  full_join(data.frame(summary(prt)$coefficients) %>% mutate(var = "Partition uncertainty"))%>%
  mutate(param = rep(c(NA,"Intercept","Year"),9))%>%
  select(var, var2, param, Estimate, Std..Error, z.value, Pr...z..)
results[-c(1,2,3)] = round(results[-c(1,2,3)], 3)
results$param[is.na(results$param)]<-results$var2[is.na(results$param)]
results%>%
  select(-var2, -var)

x = 1980:2020
probs = predict(aut, list(Year = x), type = "response")
prob_df = data.frame(var = rep("Automated_bin",length(x)),x=x,probs=probs)
real_name = c("Automated_bin", "Forecast_eval_shown", "Uncert_yn", "Iterative_yn","Archiving_yn", "End_user_yn","Drivers_published", "Null_yn", "Uncert_partition_yn")
model = c("aut", "eva","unc","ite","arc","use","dat","nul","prt")
for (i in 2:length(model)){
  probs = predict(get(model[i]), list(Year = x), type = "response")
  new = data.frame(var = rep(real_name[i],length(x)),x=x,probs=probs)
  prob_df = prob_df%>%
    full_join(new)
}

cf = complete_forecasts%>%
  select(Automated_bin, Forecast_eval_shown, Uncert_yn, Iterative_yn, Archiving_yn, End_user_yn, Drivers_published, Null_yn, Uncert_partition_yn, Year)%>%
  pivot_longer(cols = -Year)%>%
  rename(var = name, x = Year)
unique(cf$var)

#cf$Tier <- "Baseline"
#cf$Tier[cf$var %in% c("End_user_yn", "Iterative_yn", "Automated_bin")] <- "Decision Support"
#cf$Tier[cf$var %in% c("Drivers_published", "Archiving_yn","Null_yn", "Uncert_partition_yn")] <- "Science"

#prob_df$Tier <- "Baseline"
#prob_df$Tier[prob_df$var %in% c("End_user_yn", "Iterative_yn", "Automated_bin")] <- "Decision Support"
#prob_df$Tier[prob_df$var %in% c("Drivers_published", "Archiving_yn","Null_yn", "Uncert_partition_yn")] <- "Science"

labs = c("Develop automated forecasting workflows","Assess and report forecast skill", "Include data driven uncertainty","Make iterative forecasts","Archive forecasts", "Identify an end user","Make data available", "Use null model comparisons", "Partition uncertainty")
names(labs) = real_name
prob_df$var = factor(prob_df$var, levels = c("Uncert_yn", "Forecast_eval_shown", "End_user_yn", "Iterative_yn", "Automated_bin", "Drivers_published", "Archiving_yn", "Null_yn", "Uncert_partition_yn"))
cf$var = factor(cf$var, levels = c("Uncert_yn", "Forecast_eval_shown", "End_user_yn", "Iterative_yn", "Automated_bin", "Drivers_published", "Archiving_yn", "Null_yn", "Uncert_partition_yn"))
data_frame(var = c("Uncert_yn", "Forecast_eval_shown", "End_user_yn", "Iterative_yn", "Automated_bin", "Drivers_published", "Archiving_yn", "Null_yn", "Uncert_partition_yn"), )
results = results%>%
  filter(param == "Year")%>%
  select(-var2)%>%
  mutate(sig = "")
results$sig[results$Pr...z..<0.05] <- "*"
results$sig[results$Pr...z..<0.01] <- "**"
results$sig[results$Pr...z..<0.001] <- "***"
results$var <- c("Uncert_yn", "Forecast_eval_shown", "End_user_yn", "Iterative_yn", "Automated_bin", "Drivers_published", "Archiving_yn", "Null_yn", "Uncert_partition_yn")
results$var = factor(results$var)

jpeg("../Figures/best_practices)regression.jpg",res = 300,width = 8, height = 6, units = "in")
plot = prob_df%>%
  ggplot(aes(x = x, y = probs))+
  geom_point(aes(y = value), data = cf, alpha = .15, color = "blue")+
  geom_text(aes(x = 2000, y = .85, label = sig), data = results, size = 6)+
  geom_line()+
  ylim(0,1)+
  facet_wrap(~var, labeller = labeller(var = labs))+
  theme_bw()+
  ylab("Probability")+
  xlab("Year")
plot
dev.off()
plot

complete_forecasts$Horiz_days
34/44

complete_forecasts[complete_forecasts$Uncert_partition_yn==1,]
```


Time-series data
```{r}
complete_forecasts <- read.csv("../Data/complete_dataset.csv")
mean_dc<- round(mean(as.numeric(complete_forecasts$Data_coverage)/365, na.rm = T),1)
min(as.numeric(complete_forecasts$Data_coverage), na.rm = T)
max(as.numeric(complete_forecasts$Data_coverage)/365, na.rm = T)
median_dc <- round(median(as.numeric(complete_forecasts$Data_coverage)/365, na.rm = T),1)
lines = data.frame(c(mean_dc, median_dc),c(2,2))
colnames(lines) = c("x","y")
lines$label = c(paste0("Mean = ", mean_dc, " years"), paste0("Median = ",median_dc, " years"))

jpeg("../Figures/Years_of_data.jpeg", res = 300, width = 8, height = 4, units = "in")
complete_forecasts%>%
  mutate(Year = as.numeric(Year))%>%
  ggplot()+
  geom_histogram(aes(x = as.numeric(Data_coverage)/365),bins = 10)+
  geom_vline(xintercept = c(mean_dc, median_dc), color = "white")+
  geom_text(aes(x = x,y=y,label= label), data = lines, angle = 90, nudge_x = -2, hjust = 0,color = "white")+
  xlab("Years of data used to create forecasting paper")+
  ylab("Number of papers")+
  theme_light()
dev.off()

sum(as.numeric(complete_forecasts$Data_coverage)>(10*365), na.rm = T)/nrow(complete_forecasts)
complete_forecasts[as.numeric(complete_forecasts$Data_coverage)>100*365,]
```

```{r}
complete_forecasts <- read.csv("../Data/complete_dataset.csv")
time_bin = complete_forecasts
time_bin$time_bin = NA
time_bin$Time_step_days[is.na(as.numeric(time_bin$Time_step_days))] ##Use this line to check if you have all of the complicated cases covered
time_bin$time_bin[time_bin$Time_step_days == "30-150"] <- "> 1 - 12 months"
time_bin$time_bin[time_bin$Time_step_days == "1,7,15,30"] <- "1 - 30 days"
time_bin = time_bin %>%
    mutate(Time_step_days = as.numeric(Time_step_days))

time_bin$time_bin[time_bin$Time_step_days < 1] <- "< 1 day"
time_bin$time_bin[time_bin$Time_step_days == 1] <- "1 day"
time_bin$time_bin[time_bin$Time_step_days > 1 & time_bin$Time_step_days <= 7] <- "2 - 7 days"
time_bin$time_bin[time_bin$Time_step_days > 7 & time_bin$Time_step_days <= 31] <- "8 days - 1 month"
time_bin$time_bin[time_bin$Time_step_days > 31 & time_bin$Time_step_days < 365] <- "> 1 month - 1 year"
time_bin$time_bin[time_bin$Time_step_days == 365] <- "1 year"
time_bin$time_bin[time_bin$Time_step_days > 365 & time_bin$Time_step_days >= 365*5] <- "> 1 year - 5 years"
time_bin$time_bin[time_bin$Time_step_days > 365*5] <- "> 5 years"


time_bin$time_bin <- factor(time_bin$time_bin, levels = c("< 1 day", "1 day","2 - 7 days","8 days - 1 month","> 1 month - 1 year","1 year","> 1 year - 5 years", "> 5 years"))

time_bin$horiz_bin = NA
time_bin$Horiz_days[is.na(as.numeric(time_bin$Horiz_days))]
time_bin$horiz_bin[time_bin$Horiz_days == "30-150"] <- "> 1 month - 1 year"
time_bin$horiz_bin[time_bin$Horiz_days == "720 - 1800 (sites had different amounts of data)"] <- "> 1 year - 5 years"
time_bin$horiz_bin[time_bin$Horiz_days == "365 and 3650"] <- "> 5 years - 10 years"
time_bin$horiz_bin[time_bin$Horiz_days == "7, 3650"] <- "2 - 7 days" #Because timestep is listed as 1 day 
time_bin$horiz_bin[time_bin$Horiz_days == "14-77"] <- "> 1 month - 1 year"

time_bin = time_bin %>%
    mutate(Horiz_days = as.numeric(Horiz_days))
time_bin$horiz_bin[time_bin$Horiz_days < 1] <- "< 1 day"
time_bin$horiz_bin[time_bin$Horiz_days == 1] <- "1 day"
time_bin$horiz_bin[time_bin$Horiz_days > 1 & time_bin$Horiz_days <= 7] <- "2 - 7 days"
time_bin$horiz_bin[time_bin$Horiz_days > 7 & time_bin$Horiz_days <= 31] <- "8 days - 1 month"
time_bin$horiz_bin[time_bin$Horiz_days > 31 & time_bin$Horiz_days < 365] <- "> 1 month - 1 year"
time_bin$horiz_bin[time_bin$Horiz_days == 365] <- "1 year"
time_bin$horiz_bin[time_bin$Horiz_days > 365 & time_bin$Horiz_days <= 365*5] <- "> 1 year - 5 years"
time_bin$horiz_bin[time_bin$Horiz_days > 365*5] <- "> 5 years"
time_bin$horiz_bin <- factor(time_bin$horiz_bin, levels = c("< 1 day", "1 day","2 - 7 days","8 days - 1 month","> 1 month - 1 year","1 year","> 1 year - 5 years", "> 5 years"))

time_bin$time_bin <- factor(time_bin$time_bin, levels = c("< 1 day", "1 day","2 - 7 days","8 days - 1 month","> 1 month - 1 year","1 year","> 1 year - 5 years"))

time_bin_plot = time_bin%>%
  filter(!is.na(horiz_bin),
         !is.na(time_bin))

times = c("< 1 day", "1 day","2 - 7 days","8 days - 1 month","> 1 month - 1 year","1 year","> 1 year - 5 years")
horiz = c("< 1 day", "1 day","2 - 7 days","8 days - 1 month","> 1 month - 1 year","1 year","> 1 year - 5 years", "> 5 years")
fix = data.frame(horiz_bin = rep(times, length(horiz)), time_bin = rep(horiz, each = length(times)))

n = time_bin_plot%>%
  group_by(horiz_bin,time_bin)%>%
  summarise(ns = n())%>%
  mutate(color = ns>20)

jpeg("../Figures/timestep_heatmap.jpeg",res = 300,width = 7, height = 4, units = "in")
library("RColorBrewer")
cols <- brewer.pal(n = 9, name = "BuPu")
time_bin_plot%>%
  ggplot(aes(x = horiz_bin, y = time_bin), na.rm = T)+
  geom_bin2d()+
  xlab("Time horizon")+
  ylab("Time step")+
  scale_fill_gradientn(colors = cols)+
  scale_x_discrete(drop  =F)+
  scale_y_discrete(drop  =F)+
  geom_text(aes(label = ns, color = color), data = n, show.legend = F)+ #Can get rid of this by commenting out
  theme_light()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())+
  scale_color_manual(values = c("gray40","gray90"))
dev.off()
time_bin$horiz_bin
nrow(time_bin%>%filter(horiz_bin %in% c("< 1 day", "1 day","2 - 7 days","8 days - 1 month","> 1 month - 1 year","1 year")))/nrow(time_bin%>%filter(!is.na(horiz_bin)))*100
nrow(time_bin%>%filter(horiz_bin %in% c("2 - 7 days"),time_bin %in% c("1 day")))/nrow(time_bin%>%filter(!is.na(horiz_bin)))*100
nrow(time_bin%>%filter(horiz_bin %in% c("1 year"),time_bin %in% c("1 year")))/nrow(time_bin%>%filter(!is.na(horiz_bin)))*100
```

Pay attention to uncertainty 
```{r}
complete_forecasts %>%
  ggplot(aes(x = Uncert_category, fill = Iterative_yn))+
  geom_bar()+
  scale_x_discrete(limits = c("no","contains","data_driven","propagates","assimilates"))+
  theme_bw()

complete_forecasts %>%
  ggplot(aes(x = Uncert_category, fill = Uncert_technique))+
  geom_bar()+
  scale_x_discrete(limits = c("no","contains","data_driven","propagates","assimilates"))+
  theme_bw()

complete_forecasts %>%
  ggplot(aes(x = Uncert_category, fill = Uncert_partition_yn))+
  geom_bar()+
  scale_x_discrete(limits = c("no","contains","data_driven","propagates","assimilates"))+
  theme_bw()

complete_forecasts%>%
  ggplot(aes(y = Uncert_category, x = Year))+
  geom_point()+
  theme_bw()

complete_forecasts[complete_forecasts$Uncert_partition_yn == "1",] #Only 5 forecasts with partitioned uncertainty. Only 3 indicate the dominant source
complete_forecasts$R2 <- as.numeric(grepl("[R,r]2",complete_forecasts$Eval_metrics))

using_r2 = complete_forecasts%>%
  filter(R2 == 1)
write.csv(using_r2, "../Data/papers_reporting_r2.csv")
```

Use predictors related to the question 
```{r}
complete_forecasts%>%
  select(Met_covar_yn,Phys_covar_yn,Bio_covar_yn,Class)%>%
  pivot_longer(cols = 1:3)%>%
  filter(value == 1) %>%
  ggplot(aes(x=Class, fill = name))+
  geom_bar()+
  theme_bw() #This is a misleading plot because it makes it seem like we have more papers than we actually have

driver_graph = complete_forecasts%>%
  mutate(num = Met_covar_yn+Bio_covar_yn+Chem_covar_yn+Phys_covar_yn)%>%
  group_by(num)%>%
  summarise(Met = sum(Met_covar_yn), 
            Bio = sum(Bio_covar_yn), 
            Chem = sum(Chem_covar_yn), 
            Phys = sum(Phys_covar_yn), 
            Count = n())

driver_graph%>%
  pivot_longer(seq(2,ncol(driver_graph)-1))%>%
  mutate(Percent = value/sum(driver_graph$Count)*100)%>%
  ggplot(aes(x = name, y = Percent, fill = as.factor(num)))+
  geom_bar(stat = "identity")+
  labs(fill = "Number of \ncovariate classes")

driver_graph%>%
  pivot_longer(seq(2,ncol(driver_graph)-1))%>%
  mutate(Percent = value/sum(driver_graph$Count)*100)%>%
  ggplot(aes(x = as.factor(num), y = Percent, fill = name))+
  geom_bar(stat = "identity")
```


Validate using hindcasting 
```{r}
complete_forecasts%>%
  ggplot(aes(x = Iterative_yn, fill = Forecast_eval_shown))+
  geom_bar()

complete_forecasts%>%
  ggplot(aes(x = Iterative_yn, fill = Forecast_eval))+
  geom_bar()

complete_forecasts%>%
  mutate(Horiz_days = as.numeric(Horiz_days))%>%
  ggplot(aes(x = Forecast_eval, y = Horiz_days))+
  geom_point()
```